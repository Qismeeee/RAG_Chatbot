# === IMPORT CÃC THÆ¯ VIá»†N Cáº¦N THIáº¾T ===
import os
import streamlit as st
from audio_recorder_streamlit import audio_recorder
from dotenv import load_dotenv
import PyPDF2
import numpy as np
# Import the search function
from search_embeddings import search_milvus
from langchain_openai import OpenAIEmbeddings

from chat_interface import generate_answer, generate_answer_stream, speech_to_text
import asyncio
# === THIáº¾T Láº¬P GIAO DIá»†N TRANG WEB ===


def setup_page():
    st.set_page_config(
        page_title="AI Assistant",
        page_icon="ğŸ’¬",
        layout="wide"
    )

# === KHá»I Táº O á»¨NG Dá»¤NG ===


def initialize_app():
    load_dotenv()
    setup_page()

# === THANH CÃ”NG Cá»¤ BÃŠN TRÃI ===


def display_sidebar():
    st.sidebar.title("âš™ï¸ Cáº¥u hÃ¬nh")

    # Embedding Model Selection
    st.sidebar.subheader("ğŸ“š Embeddings Model")
    embedding_model = st.sidebar.radio(
        "Chá»n Embeddings Model:",
        options=["OpenAI", "Ollama"],
        index=1
    )

    # Data Source Configuration
    st.sidebar.subheader("ğŸ“š Nguá»“n dá»¯ liá»‡u")
    data_source = st.sidebar.radio(
        "Chá»n nguá»“n dá»¯ liá»‡u:",
        options=["File Local", "URL trá»±c tiáº¿p"],
        index=0
    )

    # Input fields based on the selected data source
    if data_source == "File Local":
        uploaded_file = st.sidebar.file_uploader(
            "Chá»n file:", type=["pdf", "docx", "html"])
        if uploaded_file and st.sidebar.button("Crawl dá»¯ liá»‡u"):
            with st.spinner("Äang táº£i lÃªn..."):
                upload_response = add_embedding(
                    file_id="example_id",  # Replace with actual data
                    filename=uploaded_file.name,
                    source="uploaded",
                    chunk_number=1,
                    doc_id="example_doc_id",
                    embedding=uploaded_file.read()
                )
                if upload_response:
                    st.sidebar.success("Dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c táº£i lÃªn.")
                    st.session_state.documents = get_all_embeddings()
    else:
        url = st.sidebar.text_input("Nháº­p URL:")
        collection_name = st.sidebar.text_input("TÃªn collection trong Milvus:")
        if url and collection_name and st.sidebar.button("Crawl dá»¯ liá»‡u"):
            st.sidebar.success(f"Dá»¯ liá»‡u tá»« URL: {url} Ä‘Ã£ Ä‘Æ°á»£c xá»­ lÃ½.")

    # AI Model Selection
    st.sidebar.subheader("ğŸ¤– Model AI")
    ai_model = st.sidebar.radio(
        "Chá»n AI Model Ä‘á»ƒ tráº£ lá»i:",
        options=["OpenAI GPT", "Ollama (Local)"],
        index=1
    )

# === Xá»¬ LÃ Táº¢I LÃŠN PDF ===


def handle_pdf_upload():
    uploaded_file = st.file_uploader("Chá»n file PDF", type="pdf")
    if uploaded_file is not None:
        with st.spinner("Äang xá»­ lÃ½ file PDF..."):
            try:
                pdf_reader = PyPDF2.PdfReader(uploaded_file)
                text = ""
                for page in pdf_reader.pages:
                    text += page.extract_text()
                st.success("ÄÃ£ táº£i vÃ  xá»­ lÃ½ file PDF thÃ nh cÃ´ng!")
                return text
            except Exception as e:
                st.error(f"Lá»—i khi xá»­ lÃ½ file PDF: {str(e)}")
    return None

# === GIAO DIá»†N CHAT CHÃNH ===


def setup_chat_interface():
    st.title("ğŸ’¬ AI Assistant")
    st.caption("ğŸš€ Trá»£ lÃ½ AI Ä‘Æ°á»£c há»— trá»£ bá»Ÿi LangChain vÃ  OpenAI GPT-4")

    if "messages" not in st.session_state:
        st.session_state.messages = [
            {"role": "assistant", "content": "TÃ´i cÃ³ thá»ƒ giÃºp gÃ¬ cho báº¡n?"}]


def handle_user_input(data_source):
    if data_source == "CÃ¢u há»i":
        for msg in st.session_state.messages:
            role = "assistant" if msg["role"] == "assistant" else "human"
            st.chat_message(role).write(msg["content"])
        # Create footer container for the microphone
        # footer_container = st.container()
        # with footer_container:
        col1, col2 = st.columns([3, 1])
        with col1:
            prompt = st.chat_input("HÃ£y há»i tÃ´i báº¥t cá»© Ä‘iá»u gÃ¬!")
        with col2:
            audio_bytes = audio_recorder(text="", icon_size="2x")
            print("AUDIO: ", audio_bytes)

        if audio_bytes:
            # Write the audio bytes to a file
            with st.spinner("Transcribing..."):
                webm_file_path = "temp_audio.mp3"
                with open(webm_file_path, "wb") as f:
                    f.write(audio_bytes)

                transcript = speech_to_text(webm_file_path)
                if transcript:
                    st.session_state.messages.append(
                        {"role": "user", "content": transcript})
                    with st.chat_message("user"):
                        st.write(transcript)
                    os.remove(webm_file_path)
        if prompt:
            st.session_state.messages.append(
                {"role": "human", "content": prompt})
            st.chat_message("human").write(prompt)

            # Search using the text directly
            results = search_milvus(prompt)
            print("PROMT: ", prompt)
            # print("RESULTS: ", results)

            # Format results
            formatted_results = []
            for doc in results:
                formatted_results.append({
                    "content": doc.page_content,
                    "source": doc.metadata.get("source", "Unknown"),
                    "chunk_number": doc.metadata.get("chunk_number", "Unknown")
                })

            # Display results
            response = "Káº¿t quáº£ tÃ¬m kiáº¿m:\n\n"
            for idx, result in enumerate(formatted_results, 1):
                response += f"{idx}. Nguá»“n: {result['source']}\n"
                response += f"   Äoáº¡n sá»‘: {result['chunk_number']}\n"
                response += f"   Ná»™i dung: {result['content'][:200]}...\n\n"

            st.session_state.messages.append(
                {"role": "assistant", "content": response})
            st.chat_message("assistant").write(response)
            ai_response = generate_answer(prompt, result)
            # print("AI response: ", ai_response)
            st.session_state.messages.append(
                {"role": "assistant", "content": ai_response})
            # st.chat_message("assistant").write(ai_response)
            st.chat_message("assistant").write_stream(
                ai_response)  # stream response

    elif data_source == "Táº£i lÃªn PDF":
        pdf_text = handle_pdf_upload()
        if pdf_text:
            st.session_state.messages.append(
                {"role": "human", "content": pdf_text})
            st.chat_message("human").write(pdf_text)

            # Search using the PDF text directly
            results = search_milvus(pdf_text)

            # Display results
            response = f"Top results: {results}"
            st.session_state.messages.append(
                {"role": "assistant", "content": response})
            st.chat_message("assistant").write(response)
# === HÃ€M CHÃNH ===


def main():
    initialize_app()
    data_source = setup_sidebar()
    setup_chat_interface()
    handle_user_input(data_source)


# Cháº¡y á»©ng dá»¥ng
if __name__ == "__main__":
    main()

# streamlit run streamlit_app.py
